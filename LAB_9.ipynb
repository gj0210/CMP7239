{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gj0210/CMP7239/blob/main/LAB_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 9 Feature Selection\n",
        "\n",
        "# Dr Mohamed Ihmeida\n",
        "\n",
        "# 23rd July 2025\n"
      ],
      "metadata": {
        "id": "m3ybmPRRDEM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before you start this lab, make sure you have uploaded the three csv files to your notebook.\n",
        "# The csv files (datasets) are in the lab folder."
      ],
      "metadata": {
        "id": "G7OWmvc-MaZ-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   What is Feature Selection?\n",
        "2.   Why we need Feature Selection?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fwsKRAU_Db0B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cXdwiFBdDX6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your answer here or create new text cell for that"
      ],
      "metadata": {
        "id": "zuqX_l_bDrH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.  What are the benefits of Feature Selection?\n",
        "4.  What are the methods of Feature Selection?\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVt7JrJJD01D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your answer ere"
      ],
      "metadata": {
        "id": "IjPmioStDwaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the common method of feature selection?"
      ],
      "metadata": {
        "id": "DwKzPseyEYbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your answer here\n"
      ],
      "metadata": {
        "id": "OrZ6678fEu7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The implementation without Feature Selection"
      ],
      "metadata": {
        "id": "l6fSrzUhyC0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and Preprocess Data\n",
        "\n",
        "# Load the dataset from the uploaded file\n",
        "df = pd.read_csv('Cybersecurity Intrusion Detection.csv')\n",
        "\n",
        "# Drop the session_id as it is an identifier and not a predictive feature\n",
        "df = df.drop('session_id', axis=1)\n",
        "\n",
        "# One-hot encode categorical features. This converts categories into a numerical format.\n",
        "# drop_first=True helps avoid multicollinearity.\n",
        "df_encoded = pd.get_dummies(df, columns=['protocol_type', 'encryption_used', 'browser_type'], drop_first=True)\n",
        "\n",
        "# Separate features (X) from the target variable (y)\n",
        "X = df_encoded.drop('attack_detected', axis=1)\n",
        "y = df_encoded['attack_detected']\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "# stratify=y ensures the proportion of attacks is the same in both train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Scale numerical features for KNN\n",
        "# KNN is sensitive to the scale of data, so we scale it.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Train and Evaluate K-Nearest Neighbors (KNN)\n",
        "\n",
        "print(\"--- Training K-Nearest Neighbors (KNN) ---\")\n",
        "# Initialize the classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model on the scaled training data\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_knn = knn.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the F1 score\n",
        "f1_knn = f1_score(y_test, y_pred_knn)\n",
        "print(f\"\\nF1 Score (KNN): {f1_knn:.4f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report (KNN):\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "\n",
        "# Train and Evaluate Decision Tree\n",
        "\n",
        "print(\"\\n--- Training Decision Tree Classifier ---\")\n",
        "# Initialize the classifier\n",
        "# random_state=42 ensures the results are reproducible\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the original (unscaled) training data\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate and print the F1 score\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "print(f\"\\nF1 Score (Decision Tree): {f1_dt:.4f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report (Decision Tree):\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpcO3IOfGkk5",
        "outputId": "74e95a6d-8f1d-4ec0-93ba-c60b756d4d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training K-Nearest Neighbors (KNN) ---\n",
            "\n",
            "F1 Score (KNN): 0.7296\n",
            "\n",
            "Classification Report (KNN):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.92      0.83      1582\n",
            "           1       0.86      0.63      0.73      1280\n",
            "\n",
            "    accuracy                           0.79      2862\n",
            "   macro avg       0.81      0.78      0.78      2862\n",
            "weighted avg       0.80      0.79      0.78      2862\n",
            "\n",
            "\n",
            "--- Training Decision Tree Classifier ---\n",
            "\n",
            "F1 Score (Decision Tree): 0.7936\n",
            "\n",
            "Classification Report (Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83      1582\n",
            "           1       0.80      0.79      0.79      1280\n",
            "\n",
            "    accuracy                           0.82      2862\n",
            "   macro avg       0.81      0.81      0.81      2862\n",
            "weighted avg       0.82      0.82      0.82      2862\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and Preprocess Data\n",
        "\n",
        "# Load the dataset from the uploaded file\n",
        "df = pd.read_csv('Cybersecurity Intrusion Detection.csv')\n",
        "\n",
        "# Drop the session_id as it is an identifier and not a predictive feature\n",
        "df = df.drop('session_id', axis=1)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df_encoded = pd.get_dummies(df, columns=['protocol_type', 'encryption_used', 'browser_type'], drop_first=True)\n",
        "\n",
        "# Separate features (X) from the target variable (y)\n",
        "X = df_encoded.drop('attack_detected', axis=1)\n",
        "y = df_encoded['attack_detected']\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "# stratify=y ensures the proportion of attacks is the same in both train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Scale numerical features for KNN\n",
        "# KNN is sensitive to the scale of data, so we scale it.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Train and Evaluate K-Nearest Neighbors (KNN)\n",
        "\n",
        "print(\"--- Training K-Nearest Neighbors (KNN) ---\")\n",
        "# Initialize the classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model on the scaled training data\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_knn = knn.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the F1 score\n",
        "f1_knn = f1_score(y_test, y_pred_knn)\n",
        "print(f\"\\nF1 Score (KNN): {f1_knn:.4f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report (KNN):\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "\n",
        "# Train and Evaluate Decision Tree\n",
        "\n",
        "print(\"\\n--- Training Decision Tree Classifier ---\")\n",
        "# Initialize the classifier\n",
        "# random_state=42 ensures the results are reproducible\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the original (unscaled) training data\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate and print the F1 score\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "print(f\"\\nF1 Score (Decision Tree): {f1_dt:.4f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report (Decision Tree):\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guQ9XjoQGlte",
        "outputId": "b6e07cd8-4442-4a26-9967-686f53c062cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training K-Nearest Neighbors (KNN) ---\n",
            "\n",
            "F1 Score (KNN): 0.7296\n",
            "\n",
            "Classification Report (KNN):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.92      0.83      1582\n",
            "           1       0.86      0.63      0.73      1280\n",
            "\n",
            "    accuracy                           0.79      2862\n",
            "   macro avg       0.81      0.78      0.78      2862\n",
            "weighted avg       0.80      0.79      0.78      2862\n",
            "\n",
            "\n",
            "--- Training Decision Tree Classifier ---\n",
            "\n",
            "F1 Score (Decision Tree): 0.7936\n",
            "\n",
            "Classification Report (Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83      1582\n",
            "           1       0.80      0.79      0.79      1280\n",
            "\n",
            "    accuracy                           0.82      2862\n",
            "   macro avg       0.81      0.81      0.81      2862\n",
            "weighted avg       0.82      0.82      0.82      2862\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ## The implementation with Feature Selection"
      ],
      "metadata": {
        "id": "rpSPH8mbyJ-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first work\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "\n",
        "df = pd.read_csv('Cybersecurity Intrusion Detection.csv')\n",
        "df = df.drop('session_id', axis=1)\n",
        "\n",
        "\n",
        "df_encoded = pd.get_dummies(df, columns=['protocol_type', 'encryption_used', 'browser_type'], drop_first=True)\n",
        "\n",
        "\n",
        "X = df_encoded.drop('attack_detected', axis=1)\n",
        "y = df_encoded['attack_detected']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=20, stratify=y)\n",
        "\n",
        "k_best_count = 3\n",
        "selector = SelectKBest(score_func=chi2, k=k_best_count)\n",
        "\n",
        "\n",
        "X_train_kbest = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "\n",
        "X_test_kbest = selector.transform(X_test)\n",
        "\n",
        "\n",
        "selected_features_mask = selector.get_support()\n",
        "selected_features_names = X_train.columns[selected_features_mask]\n",
        "print(f\"Selected the top {k_best_count} features: {list(selected_features_names)}\\n\")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_kbest_scaled = scaler.fit_transform(X_train_kbest)\n",
        "X_test_kbest_scaled = scaler.transform(X_test_kbest)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- Training K-Nearest Neighbors (KNN) on Selected Features ---\")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "\n",
        "knn.fit(X_train_kbest_scaled, y_train)\n",
        "\n",
        "\n",
        "y_pred_knn = knn.predict(X_test_kbest_scaled)\n",
        "\n",
        "\n",
        "f1_knn = f1_score(y_test, y_pred_knn)\n",
        "print(f\"\\nF1 Score (KNN with Feature Selection): {f1_knn:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nClassification Report (KNN with Feature Selection):\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Training Decision Tree on Selected Features ---\")\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=20)\n",
        "\n",
        "\n",
        "dt.fit(X_train_kbest, y_train)\n",
        "\n",
        "y_pred_dt = dt.predict(X_test_kbest)\n",
        "\n",
        "\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "print(f\"\\nF1 Score (Decision Tree with Feature Selection): {f1_dt:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Decision Tree with Feature Selection):\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld6oeHrOHvqy",
        "outputId": "ee93aabe-3f30-415a-dc7c-cd884b38dccb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected the top 3 features: ['login_attempts', 'session_duration', 'failed_logins']\n",
            "\n",
            "--- Training K-Nearest Neighbors (KNN) on Selected Features ---\n",
            "\n",
            "F1 Score (KNN with Feature Selection): 0.7188\n",
            "\n",
            "Classification Report (KNN with Feature Selection):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.96      0.84      1582\n",
            "           1       0.91      0.59      0.72      1280\n",
            "\n",
            "    accuracy                           0.79      2862\n",
            "   macro avg       0.83      0.77      0.78      2862\n",
            "weighted avg       0.82      0.79      0.78      2862\n",
            "\n",
            "\n",
            "--- Training Decision Tree on Selected Features ---\n",
            "\n",
            "F1 Score (Decision Tree with Feature Selection): 0.6889\n",
            "\n",
            "Classification Report (Decision Tree with Feature Selection):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.74      0.74      1582\n",
            "           1       0.68      0.69      0.69      1280\n",
            "\n",
            "    accuracy                           0.72      2862\n",
            "   macro avg       0.72      0.72      0.72      2862\n",
            "weighted avg       0.72      0.72      0.72      2862\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Comapre the F1 score for KNN and Decision Tree with and without feature selection.\n",
        "\n",
        "7. Can you draw `bar-chat` for each F1 score?\n",
        "\n",
        "8. Try to change k_best_count and observe F1-Score\n",
        "\n",
        "9. Consider the number of features with F1-Score. Draw the realtionship between k_best_count and F1-Score."
      ],
      "metadata": {
        "id": "VM8-sTEbIYde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add your answer here"
      ],
      "metadata": {
        "id": "t-CoY45VIx7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ## The implementation without Feature Selection with NSL-KDD dataset"
      ],
      "metadata": {
        "id": "wtIOCcEyyNxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load Your Provided Train and Test Data\n",
        "df_train = pd.read_csv('kdd_train.csv')\n",
        "df_test = pd.read_csv('kdd_test.csv')\n",
        "# print(\"Successfully loaded kdd_train.csv and kdd_test.csv.\")\n",
        "\n",
        "\n",
        "# Preprocess Data Consistently\n",
        "\n",
        "# Store original lengths to split them back later\n",
        "train_len = len(df_train)\n",
        "\n",
        "# Combine for consistent encoding\n",
        "df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "\n",
        "# Use LabelEncoder for categorical features\n",
        "# This is simpler and sufficient for tree-based models\n",
        "for col in ['protocol_type', 'service', 'flag']:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Create a binary target variable: 1 for any attack, 0 for normal\n",
        "df['attack'] = df['labels'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "df = df.drop('labels', axis=1)\n",
        "\n",
        "# Separate features from target\n",
        "X = df.drop('attack', axis=1)\n",
        "y = df['attack']\n",
        "\n",
        "# Inject Noise Features\n",
        "# We add 60 columns of random noise to simulate a high-dimensional environment\n",
        "num_noise_features = 60\n",
        "for i in range(num_noise_features):\n",
        "    noise = np.random.rand(len(X))\n",
        "    X[f'noise_{i}'] = noise\n",
        "\n",
        "print(f\"Total features after adding noise: {X.shape[1]}\")\n",
        "\n",
        "# Separate back into Training and Testing Sets\n",
        "X_train = X.iloc[:train_len]\n",
        "X_test = X.iloc[train_len:]\n",
        "y_train = y.iloc[:train_len]\n",
        "y_test = y.iloc[train_len:]\n",
        "\n",
        "\n",
        "# Scale Data and Train Models\n",
        "\n",
        "# Scale data for KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "print(\"\\n--- Training K-Nearest Neighbors (Without Feature Selection) ---\")\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_knn = knn.predict(X_test_scaled)\n",
        "f1_knn = f1_score(y_test, y_pred_knn)\n",
        "print(f\"F1 Score (KNN on noisy data): {f1_knn:.4f}\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Decision Tree\n",
        "print(\"\\n--- Training Decision Tree (Without Feature Selection) ---\")\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train) # Use unscaled data for tree\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "print(f\"F1 Score (Decision Tree on noisy data): {f1_dt:.4f}\")\n",
        "print(classification_report(y_test, y_pred_dt))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded kdd_train.csv and kdd_test.csv.\n",
            "Total features after adding noise: 101\n",
            "\n",
            "--- Training K-Nearest Neighbors (Without Feature Selection) ---\n",
            "F1 Score (KNN on noisy data): 0.9017\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.99      0.92     11245\n",
            "           1       0.99      0.83      0.90     11299\n",
            "\n",
            "    accuracy                           0.91     22544\n",
            "   macro avg       0.92      0.91      0.91     22544\n",
            "weighted avg       0.92      0.91      0.91     22544\n",
            "\n",
            "\n",
            "--- Training Decision Tree (Without Feature Selection) ---\n",
            "F1 Score (Decision Tree on noisy data): 0.9268\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.99      0.93     11245\n",
            "           1       0.99      0.87      0.93     11299\n",
            "\n",
            "    accuracy                           0.93     22544\n",
            "   macro avg       0.94      0.93      0.93     22544\n",
            "weighted avg       0.94      0.93      0.93     22544\n",
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxa24d7pvPus",
        "outputId": "01fea611-68f5-4a38-e3bd-168d117b1eeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ## ## The implementation with Feature Selection with NSL-KDD dataset"
      ],
      "metadata": {
        "id": "bvBCZ09IyWnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif"
      ],
      "metadata": {
        "id": "JQldW2-G-xND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Your Provided Train and Test Data\n",
        "df_train = pd.read_csv('kdd_train.csv')  # load the train data here\n",
        "df_test = pd.read_csv('kdd_test.csv')  # load the test data here\n",
        "print(\"Successfully loaded kdd_train.csv and kdd_test.csv.\")\n",
        "\n",
        "\n",
        "# Preprocess Data Consistently\n",
        "# We process train and test sets separately to avoid data leakage during encoding,\n",
        "# but for simplicity with LabelEncoder, we'll fit on train and transform both.\n",
        "\n",
        "# Create a binary target variable: 1 for any attack, 0 for normal\n",
        "y_train = df_train['labels'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "y_test = df_test['labels'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "X_train = df_train.drop('labels', axis=1)\n",
        "X_test = df_test.drop('labels', axis=1)\n",
        "\n",
        "# Use LabelEncoder for categorical features, fitting only on training data\n",
        "for col in ['protocol_type', 'service', 'flag']:\n",
        "    if col in X_train.columns:\n",
        "        le = LabelEncoder()\n",
        "        X_train[col] = le.fit_transform(X_train[col])\n",
        "        # Handle new categories in test data that were not in train data\n",
        "        X_test[col] = X_test[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n",
        "        le.classes_ = np.append(le.classes_, '<unknown>')\n",
        "        X_test[col] = le.transform(X_test[col])\n",
        "\n",
        "\n",
        "# print(f\"Total features before selection: {X_train.shape[1]}\")\n",
        "\n",
        "# Apply Feature Selection CORRECTLY\n",
        "# We will select the 30 best features to filter out the noise.\n",
        "k_best_count = 30\n",
        "selector = SelectKBest(score_func=f_classif, k=k_best_count)\n",
        "\n",
        "# Fit the selector ONLY on the training data\n",
        "selector.fit(X_train, y_train)\n",
        "\n",
        "# Transform both the training and testing data using the fitted selector\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
        "\n",
        "\n",
        "# Scale Data and Train Models\n",
        "\n",
        "# Scale the selected data for KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
        "X_test_scaled = scaler.transform(X_test_selected)\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "print(\"\\n--- Training K-Nearest Neighbors (With Feature Selection) ---\")\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_knn = knn.predict(X_test_scaled)\n",
        "f1_knn = f1_score(y_test, y_pred_knn)\n",
        "print(f\"F1 Score (KNN on cleaned data): {f1_knn:.4f}\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# Decision Tree\n",
        "print(\"\\n--- Training Decision Tree (With Feature Selection) ---\")\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train_selected, y_train) # Use unscaled, selected data for tree\n",
        "y_pred_dt = dt.predict(X_test_selected)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "print(f\"F1 Score (Decision Tree on cleaned data): {f1_dt:.4f}\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHPTTEIyxDHL",
        "outputId": "1b0dc85f-dfc0-4349-9a12-c517a6907aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded kdd_train.csv and kdd_test.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [19] are constant.\n",
            "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
            "  f = msb / msw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features after selection: 15\n",
            "\n",
            "--- Training K-Nearest Neighbors (With Feature Selection) ---\n",
            "F1 Score (KNN on cleaned data): 0.9339\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94     11245\n",
            "           1       0.99      0.89      0.93     11299\n",
            "\n",
            "    accuracy                           0.94     22544\n",
            "   macro avg       0.94      0.94      0.94     22544\n",
            "weighted avg       0.94      0.94      0.94     22544\n",
            "\n",
            "\n",
            "--- Training Decision Tree (With Feature Selection) ---\n",
            "F1 Score (Decision Tree on cleaned data): 0.9309\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.99      0.94     11245\n",
            "           1       0.99      0.88      0.93     11299\n",
            "\n",
            "    accuracy                           0.93     22544\n",
            "   macro avg       0.94      0.93      0.93     22544\n",
            "weighted avg       0.94      0.93      0.93     22544\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Comapre the F1 score for KNN and Decision Tree with and without feature selection.\n",
        "\n",
        "11. Can you draw `bar-chat` for each F1 score?\n",
        "\n",
        "12. Try to change k_best_count and observe F1-Score\n",
        "\n",
        "13. Consider the number of features with F1-Score. Draw the realtionship between k_best_count and F1-Score."
      ],
      "metadata": {
        "id": "9WEqZv8GKr1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your answer here\n"
      ],
      "metadata": {
        "id": "94Hn9PCq_mfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. if you could compute the time between the approaches with and without feature selection. This will very good!\n",
        "\n",
        "15. Draw the bar-chart for both approaches times."
      ],
      "metadata": {
        "id": "HU4_IloXLCyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Add your answer here"
      ],
      "metadata": {
        "id": "k0GV9ejWMPf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Can you summarise the benefits and challenges for feature selection from your experience? (just couple of sentences from your own words)"
      ],
      "metadata": {
        "id": "FOFi-pb4Lp0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your answer here"
      ],
      "metadata": {
        "id": "qZbwJFjyLCAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# 1. Create the selector to find the top 10 features\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "\n",
        "# 2. Fit it on training data to find the best features\n",
        "selector.fit(X_train, y_train)\n",
        "\n",
        "# 3. Transform the data to keep only the selected features\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)"
      ],
      "metadata": {
        "id": "xYykZilla4_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb76128"
      },
      "source": [
        "1. **What is Feature Selection?**\n",
        "\n",
        "Feature selection is the process of selecting a subset of relavant features(variables,predictors) for use in model construction. its a way to reduce the number of input variables when developing a predictive model.\n",
        "\n",
        "\n",
        "2. **Why do we need Feature Selection?**\n",
        "\n",
        "Add your answers here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a21bcd"
      },
      "source": [
        "1. **What is Feature Selection?**\n",
        "\n",
        "Feature Selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. It's a way to reduce the number of input variables when developing a predictive model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}